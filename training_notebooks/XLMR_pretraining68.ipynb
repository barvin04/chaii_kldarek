{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_bEWP-wNsuQ5"
   },
   "outputs": [],
   "source": [
    "EXP_NAME = 'pretraining_XLMR_68'\n",
    "FOLDER_NAME = EXP_NAME\n",
    "SEED = 68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xiroEQT-TSmS"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from datasets import Dataset\n",
    "from sklearn import model_selection \n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from transformers import default_data_collator\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import gc\n",
    "\n",
    "import re\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jIjUdvuFTSmU"
   },
   "outputs": [],
   "source": [
    "model_checkpoint = 'xlm-roberta-large'\n",
    "batch_size = 8\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "max_length = 384\n",
    "doc_stride = 192\n",
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "n_folds = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "seV45044STUq"
   },
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TY___ATMTSmV"
   },
   "outputs": [],
   "source": [
    "def create_folds(data, num_splits):\n",
    "    data[\"kfold\"] = -1\n",
    "    kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=2021)\n",
    "    for fold_num, (t_, v_) in enumerate(kf.split(X=data, y=data.language.values)):\n",
    "        data.loc[v_, \"kfold\"] = fold_num\n",
    "    return data\n",
    "\n",
    "def convert_answers(row):\n",
    "    return {\"answer_start\": [row[0]], \"text\": [row[1]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sampling(examples, ratio=0.1):\n",
    "    \n",
    "    def _sample(pos):\n",
    "        if pos != 0: return True\n",
    "        else: return random.random() < ratio\n",
    "                \n",
    "    indices = [i for i,x in enumerate(examples['start_positions']) if _sample(x)]\n",
    "\n",
    "    for key in examples.keys():\n",
    "        examples[key] = [x for i,x in enumerate(examples[key]) if i in indices]\n",
    "        \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (/home/darek/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "311eb379e4dd42f0afae30f5e8383b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "squad = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HaShvVOxfceM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11650\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6553242570085056655</td>\n",
       "      <td>\\nవేపచెట్టు, వేపాకు, వేపపూత ఇలా వేపచెట్టునుండి...</td>\n",
       "      <td>వేప చెట్టు యొక్క శాస్త్రీయ నామం ఏమిటి?</td>\n",
       "      <td>Azadirachta indica</td>\n",
       "      <td>702</td>\n",
       "      <td>telugu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                            context  \\\n",
       "0  6553242570085056655  \\nవేపచెట్టు, వేపాకు, వేపపూత ఇలా వేపచెట్టునుండి...   \n",
       "\n",
       "                                 question         answer_text  answer_start  \\\n",
       "0  వేప చెట్టు యొక్క శాస్త్రీయ నామం ఏమిటి?  Azadirachta indica           702   \n",
       "\n",
       "  language  \n",
       "0   telugu  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tydi = pd.read_csv('tydiqa_train.csv.zip')\n",
    "tydi_bete_all = tydi[(tydi.language == 'bengali') | (tydi.language == 'telugu') | (tydi.language == 'english')].reset_index(drop=True)\n",
    "print(len(tydi_bete_all))\n",
    "tydi_bete_all.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10658\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_text</th>\n",
       "      <th>context</th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>p0</th>\n",
       "      <th>p1</th>\n",
       "      <th>alen</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103 Hz</td>\n",
       "      <td>\\n\\nThe hertz (symbol: Hz) is the derived unit...</td>\n",
       "      <td>532428134682627320</td>\n",
       "      <td>1 kilohertz is equal to how many hertz?</td>\n",
       "      <td>324</td>\n",
       "      <td>-0.469617</td>\n",
       "      <td>0.347219</td>\n",
       "      <td>6</td>\n",
       "      <td>{'answer_start': [324], 'text': ['103 Hz']}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  answer_text                                            context  \\\n",
       "0      103 Hz  \\n\\nThe hertz (symbol: Hz) is the derived unit...   \n",
       "\n",
       "                   id                                 question  answer_start  \\\n",
       "0  532428134682627320  1 kilohertz is equal to how many hertz?           324   \n",
       "\n",
       "         p0        p1  alen                                      answers  \n",
       "0 -0.469617  0.347219     6  {'answer_start': [324], 'text': ['103 Hz']}  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nq = pd.read_csv('nq_small.csv.zip')\n",
    "nq = nq.rename(columns={'answer':'answer_text'})\n",
    "nq[\"answers\"] = nq[[\"answer_start\", \"answer_text\"]].apply(convert_answers, axis=1)\n",
    "print(len(nq))\n",
    "nq.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115,
     "referenced_widgets": [
      "d26f33dd4947429f9e7632dacb298a55",
      "a71ec336fa884547b53fd770ea8ba276"
     ]
    },
    "id": "gHiADvpBUmH_",
    "outputId": "9e927209-9282-46e2-b96e-3e29dc4fefc4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96318cea80fe425aba9bcd98fa4419d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2714062d1b524fe9a6ad33f55c3fa757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f4b977cd5504e439568ed5d3c8500f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33284c3eea8449bfbf6d5cbae93c4da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2af52ad643c64e5187c31b6a40cf1541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/darek/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-49f4dc246be22605.arrow\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "threeds = pd.read_csv('chaii-mlqa-xquad-5folds.csv')\n",
    "hindi = threeds[threeds.src != 'chaii'].reset_index(drop=True)\n",
    "chaii = threeds[threeds.src == 'chaii'].reset_index(drop=True)\n",
    "chaii = pd.merge(chaii, train[['id', 'context', 'question']], how='left', on=['context', 'question'])\n",
    "del chaii['fold']\n",
    "chaii = create_folds(chaii, 3)\n",
    "\n",
    "chaii_train = chaii[chaii.kfold != fold]\n",
    "chaii_valid = chaii[chaii.kfold == fold]\n",
    "\n",
    "chaii_train[\"answers\"] = chaii_train[[\"answer_start\", \"answer_text\"]].apply(convert_answers, axis=1)\n",
    "chaii_valid[\"answers\"] = chaii_valid[[\"answer_start\", \"answer_text\"]].apply(convert_answers, axis=1)\n",
    "hindi[\"answers\"] = hindi[[\"answer_start\", \"answer_text\"]].apply(convert_answers, axis=1)\n",
    "tydi_bete_all[\"answers\"] = tydi_bete_all[[\"answer_start\", \"answer_text\"]].apply(convert_answers, axis=1)\n",
    "\n",
    "chaii_train_ds = Dataset.from_pandas(chaii_train)\n",
    "chaii_valid_ds = Dataset.from_pandas(chaii_valid)\n",
    "hindi_ds = Dataset.from_pandas(hindi)\n",
    "tydi_bete_all_ds = Dataset.from_pandas(tydi_bete_all)\n",
    "nq_ds = Dataset.from_pandas(nq)\n",
    "\n",
    "tokenized_tydi_bete_all_strid = tydi_bete_all_ds.map(prepare_train_features, batched=True, remove_columns=tydi_bete_all_ds.column_names)\n",
    "tokenized_nq_strid = nq_ds.map(prepare_train_features, batched=True, remove_columns=nq_ds.column_names)\n",
    "tokenized_chaii_train_strid = chaii_train_ds.map(prepare_train_features, batched=True, remove_columns=chaii_train_ds.column_names)\n",
    "tokenized_chaii_valid_strid = chaii_valid_ds.map(prepare_train_features, batched=True, remove_columns=chaii_valid_ds.column_names)\n",
    "tokenized_hindi_strid = hindi_ds.map(prepare_train_features, batched=True, remove_columns=hindi_ds.column_names)\n",
    "tokenized_squad = squad['train'].map(prepare_train_features, batched=True, remove_columns=squad['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ac0b7cf3704ec3abc0a57b765f932d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41161 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "018541f8c80144b788a07acd3c7d9ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41161 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8741e4dadc6949f6a5839838369643cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39302 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efcd8f08e2354ed387f2660ff29dc18f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1598 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d54b0df1defb4c6c91ddf3771b60a96a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1598 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_tydi_bete_all_strid1 = tokenized_tydi_bete_all_strid.map(negative_sampling, batched=True, batch_size=8)\n",
    "tokenized_tydi_bete_all_strid2 = tokenized_tydi_bete_all_strid.map(negative_sampling, batched=True, batch_size=8)\n",
    "tokenized_nq_strid = tokenized_nq_strid.map(negative_sampling, fn_kwargs={'ratio':0.06}, batched=True, batch_size=8)\n",
    "tokenized_chaii_train_strid1 = tokenized_chaii_train_strid.map(negative_sampling, fn_kwargs={'ratio':0.1}, batched=True, batch_size=8)\n",
    "tokenized_chaii_train_strid2 = tokenized_chaii_train_strid.map(negative_sampling, fn_kwargs={'ratio':0.2}, batched=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "ep1 = concatenate_datasets([tokenized_tydi_bete_all_strid1, tokenized_squad, tokenized_hindi_strid, tokenized_chaii_train_strid1]).shuffle(seed=67)\n",
    "ep2 = concatenate_datasets([tokenized_tydi_bete_all_strid2, tokenized_nq_strid, tokenized_hindi_strid, tokenized_chaii_train_strid2]).shuffle(seed=42)\n",
    "tokenized_train_all = concatenate_datasets([ep1, ep2, tokenized_chaii_train_strid])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255016"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, IterableDataset, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers.file_utils import is_datasets_available\n",
    "\n",
    "class MyTrainer(Trainer):\n",
    "\n",
    "    def get_train_dataloader(self) -> DataLoader:\n",
    "        if self.train_dataset is None:\n",
    "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "\n",
    "        train_dataset = self.train_dataset\n",
    "        if is_datasets_available() and isinstance(train_dataset, Dataset):\n",
    "            train_dataset = self._remove_unused_columns(train_dataset, description=\"training\")\n",
    "\n",
    "        train_sampler = SequentialSampler(self.train_dataset)\n",
    "\n",
    "        return DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.args.train_batch_size,\n",
    "            sampler=train_sampler,\n",
    "            collate_fn=self.data_collator,\n",
    "            drop_last=self.args.dataloader_drop_last,\n",
    "            num_workers=self.args.dataloader_num_workers,\n",
    "            pin_memory=self.args.dataloader_pin_memory,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qrhwPomabB4v",
    "outputId": "ee860ff0-9afb-443a-c298-8514a6579c94"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "8RXHF7BAIfwg"
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    f\"chaii-qa-{EXP_NAME}\",\n",
    "    evaluation_strategy = \"steps\",\n",
    "    logging_strategy = \"steps\",\n",
    "    logging_steps = 1000,\n",
    "    save_steps = 1000,\n",
    "    save_strategy = \"steps\",\n",
    "    learning_rate=1e-5,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_ratio=0.2,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    report_to='none',\n",
    "    save_total_limit=15\n",
    ")\n",
    "\n",
    "data_collator = default_data_collator\n",
    "\n",
    "trainer = MyTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_train_all,\n",
    "    eval_dataset=tokenized_chaii_valid_strid,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    },
    "id": "KG1m0k2vZ8OU",
    "outputId": "ef71518c-69f3-412e-f273-bd4dc985438c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 255016\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 7969\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7969' max='7969' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7969/7969 3:03:49, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.830500</td>\n",
       "      <td>0.511914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.959400</td>\n",
       "      <td>0.309082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.823300</td>\n",
       "      <td>0.479905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.752100</td>\n",
       "      <td>0.369783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.722200</td>\n",
       "      <td>0.307774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.656700</td>\n",
       "      <td>0.287275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.611300</td>\n",
       "      <td>0.288767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 6727\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to chaii-qa-pretraining_XLMR_68/checkpoint-1000\n",
      "Configuration saved in chaii-qa-pretraining_XLMR_68/checkpoint-1000/config.json\n",
      "Model weights saved in chaii-qa-pretraining_XLMR_68/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in chaii-qa-pretraining_XLMR_68/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in chaii-qa-pretraining_XLMR_68/checkpoint-1000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6727\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to chaii-qa-pretraining_XLMR_68/checkpoint-2000\n",
      "Configuration saved in chaii-qa-pretraining_XLMR_68/checkpoint-2000/config.json\n",
      "Model weights saved in chaii-qa-pretraining_XLMR_68/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in chaii-qa-pretraining_XLMR_68/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in chaii-qa-pretraining_XLMR_68/checkpoint-2000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6727\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to chaii-qa-pretraining_XLMR_68/checkpoint-3000\n",
      "Configuration saved in chaii-qa-pretraining_XLMR_68/checkpoint-3000/config.json\n",
      "Model weights saved in chaii-qa-pretraining_XLMR_68/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in chaii-qa-pretraining_XLMR_68/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in chaii-qa-pretraining_XLMR_68/checkpoint-3000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6727\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to chaii-qa-pretraining_XLMR_68/checkpoint-4000\n",
      "Configuration saved in chaii-qa-pretraining_XLMR_68/checkpoint-4000/config.json\n",
      "Model weights saved in chaii-qa-pretraining_XLMR_68/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in chaii-qa-pretraining_XLMR_68/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in chaii-qa-pretraining_XLMR_68/checkpoint-4000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6727\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to chaii-qa-pretraining_XLMR_68/checkpoint-5000\n",
      "Configuration saved in chaii-qa-pretraining_XLMR_68/checkpoint-5000/config.json\n",
      "Model weights saved in chaii-qa-pretraining_XLMR_68/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in chaii-qa-pretraining_XLMR_68/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in chaii-qa-pretraining_XLMR_68/checkpoint-5000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6727\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to chaii-qa-pretraining_XLMR_68/checkpoint-6000\n",
      "Configuration saved in chaii-qa-pretraining_XLMR_68/checkpoint-6000/config.json\n",
      "Model weights saved in chaii-qa-pretraining_XLMR_68/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in chaii-qa-pretraining_XLMR_68/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in chaii-qa-pretraining_XLMR_68/checkpoint-6000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6727\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to chaii-qa-pretraining_XLMR_68/checkpoint-7000\n",
      "Configuration saved in chaii-qa-pretraining_XLMR_68/checkpoint-7000/config.json\n",
      "Model weights saved in chaii-qa-pretraining_XLMR_68/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in chaii-qa-pretraining_XLMR_68/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in chaii-qa-pretraining_XLMR_68/checkpoint-7000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to pretraining_XLMR_68/final\n",
      "Configuration saved in pretraining_XLMR_68/final/config.json\n",
      "Model weights saved in pretraining_XLMR_68/final/pytorch_model.bin\n",
      "tokenizer config file saved in pretraining_XLMR_68/final/tokenizer_config.json\n",
      "Special tokens file saved in pretraining_XLMR_68/final/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(f\"{FOLDER_NAME}/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "M5Nk8A-UZ8Uv"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def prepare_validation_features(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # We keep the example_id that gave us this feature and we will store the offset mappings.\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
    "        # position is part of the context or not.\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples\n",
    "\n",
    "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    # Build a map example to its corresponding features.\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    # The dictionaries we have to fill.\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    # Logging.\n",
    "    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
    "\n",
    "    # Let's loop over all the examples!\n",
    "    for example_index, example in enumerate(tqdm(examples)):\n",
    "        # Those are the indices of the features associated to the current example.\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        min_null_score = None # Only used if squad_v2 is True.\n",
    "        valid_answers = []\n",
    "        \n",
    "        context = example[\"context\"]\n",
    "        # Looping through all the features associated to the current example.\n",
    "        for feature_index in feature_indices:\n",
    "            # We grab the predictions of the model for this feature.\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
    "            # context.\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            # Update minimum null prediction.\n",
    "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "            if min_null_score is None or min_null_score < feature_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "\n",
    "            # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
    "                    # to part of the input_ids that are not in the context.\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    valid_answers.append(\n",
    "                        {\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"text\": context[start_char: end_char]\n",
    "                        }\n",
    "                    )\n",
    "        \n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else:\n",
    "            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
    "            # failure.\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "        \n",
    "        # Let's pick our final answer: the best one or the null answer (only for squad_v2)\n",
    "        predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def jaccard(row): \n",
    "    str1 = row[0]\n",
    "    str2 = row[1]\n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "def postuning(s):\n",
    "    s = \" \".join(s.split())\n",
    "    s = s.strip(punctuation)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "628b08eb7cd94fa8ad15ea37d91a1c8b"
     ]
    },
    "id": "QgZkwma1Z8lp",
    "outputId": "056c927a-21c9-4071-d1f0-5088feaaa5b7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3443969a1ac8465a86cbea3495080c3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cbc891b7e6440eb800bd7c6abb7b604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6727 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "validation_features = chaii_valid_ds.map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=chaii_valid_ds.column_names\n",
    ")\n",
    "\n",
    "valid_feats_small = validation_features.map(lambda example: example, remove_columns=['example_id', 'offset_mapping'])\n",
    "\n",
    "max_answer_length = 30\n",
    "\n",
    "examples = chaii_valid_ds\n",
    "features = validation_features\n",
    "\n",
    "example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "features_per_example = collections.defaultdict(list)\n",
    "for i, feature in enumerate(features):\n",
    "    features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "references = [{\"id\": ex[\"id\"], \"context\": ex[\"context\"], \"question\": ex[\"question\"], \"answer\": ex[\"answer_text\"]} for ex in chaii_valid_ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 6727\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='841' max='841' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [841/841 01:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing 371 example predictions split into 6727 features.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b8919aeed0499fb663549a7e80ff86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/371 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.7062603202428, 0.7067095565410928)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final\n",
    "raw_predictions = trainer.predict(valid_feats_small)\n",
    "final_predictions = postprocess_qa_predictions(chaii_valid_ds, validation_features, raw_predictions.predictions)\n",
    "res = pd.DataFrame(references)\n",
    "res['prediction'] = res['id'].apply(lambda r: final_predictions[r])\n",
    "res['jaccard'] = res[['answer', 'prediction']].apply(jaccard, axis=1)\n",
    "res['postuned'] = res['prediction'].apply(postuning)\n",
    "res['pjaccard'] = res[['answer', 'postuned']].apply(jaccard, axis=1)\n",
    "res.jaccard.mean(), res.pjaccard.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "qe08GBlohDjj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[0m\u001b[01;34mcheckpoint-1000\u001b[0m/  \u001b[01;34mcheckpoint-3000\u001b[0m/  \u001b[01;34mcheckpoint-5000\u001b[0m/  \u001b[01;34mcheckpoint-7000\u001b[0m/\r\n",
      "\u001b[01;34mcheckpoint-2000\u001b[0m/  \u001b[01;34mcheckpoint-4000\u001b[0m/  \u001b[01;34mcheckpoint-6000\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls chaii-qa-pretraining_XLMR_68/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "config.json              special_tokens_map.json  training_args.bin\r\n",
      "pytorch_model.bin        tokenizer.json\r\n",
      "sentencepiece.bpe.model  tokenizer_config.json\r\n"
     ]
    }
   ],
   "source": [
    "ls pretraining_XLMR_68/final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file chaii-qa-pretraining_XLMR_68/checkpoint-4000/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file chaii-qa-pretraining_XLMR_68/checkpoint-4000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForQuestionAnswering.\n",
      "\n",
      "All the weights of XLMRobertaForQuestionAnswering were initialized from the model checkpoint at chaii-qa-pretraining_XLMR_68/checkpoint-4000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForQuestionAnswering for predictions without further training.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 6727\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='841' max='841' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [841/841 01:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing 371 example predictions split into 6727 features.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "442c7f0840944cab9011db33d8b71b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/371 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.6765501457213048, 0.6805932724059409)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint = 'chaii-qa-pretraining_XLMR_68/checkpoint-4000'\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "raw_predictions = trainer.predict(valid_feats_small)\n",
    "final_predictions = postprocess_qa_predictions(chaii_valid_ds, validation_features, raw_predictions.predictions)\n",
    "res = pd.DataFrame(references)\n",
    "res['prediction'] = res['id'].apply(lambda r: final_predictions[r])\n",
    "res['jaccard'] = res[['answer', 'prediction']].apply(jaccard, axis=1)\n",
    "res['postuned'] = res['prediction'].apply(postuning)\n",
    "res['pjaccard'] = res[['answer', 'postuned']].apply(jaccard, axis=1)\n",
    "res.jaccard.mean(), res.pjaccard.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file chaii-qa-pretraining_XLMR_68/checkpoint-5000/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file chaii-qa-pretraining_XLMR_68/checkpoint-5000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForQuestionAnswering.\n",
      "\n",
      "All the weights of XLMRobertaForQuestionAnswering were initialized from the model checkpoint at chaii-qa-pretraining_XLMR_68/checkpoint-5000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForQuestionAnswering for predictions without further training.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 6727\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='841' max='841' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [841/841 01:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing 371 example predictions split into 6727 features.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b9b3092400f43018f66f3e2d8b587ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/371 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.6667899523357744, 0.6699346064238247)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint = 'chaii-qa-pretraining_XLMR_68/checkpoint-5000'\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "raw_predictions = trainer.predict(valid_feats_small)\n",
    "final_predictions = postprocess_qa_predictions(chaii_valid_ds, validation_features, raw_predictions.predictions)\n",
    "res = pd.DataFrame(references)\n",
    "res['prediction'] = res['id'].apply(lambda r: final_predictions[r])\n",
    "res['jaccard'] = res[['answer', 'prediction']].apply(jaccard, axis=1)\n",
    "res['postuned'] = res['prediction'].apply(postuning)\n",
    "res['pjaccard'] = res[['answer', 'postuned']].apply(jaccard, axis=1)\n",
    "res.jaccard.mean(), res.pjaccard.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file chaii-qa-pretraining_XLMR_68/checkpoint-6000/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file chaii-qa-pretraining_XLMR_68/checkpoint-6000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForQuestionAnswering.\n",
      "\n",
      "All the weights of XLMRobertaForQuestionAnswering were initialized from the model checkpoint at chaii-qa-pretraining_XLMR_68/checkpoint-6000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForQuestionAnswering for predictions without further training.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 6727\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='841' max='841' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [841/841 01:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing 371 example predictions split into 6727 features.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edc3c4b2f3dc4465aa536c7555f4b5f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/371 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.6871788626168679, 0.6876280989151609)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint = 'chaii-qa-pretraining_XLMR_68/checkpoint-6000'\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "raw_predictions = trainer.predict(valid_feats_small)\n",
    "final_predictions = postprocess_qa_predictions(chaii_valid_ds, validation_features, raw_predictions.predictions)\n",
    "res = pd.DataFrame(references)\n",
    "res['prediction'] = res['id'].apply(lambda r: final_predictions[r])\n",
    "res['jaccard'] = res[['answer', 'prediction']].apply(jaccard, axis=1)\n",
    "res['postuned'] = res['prediction'].apply(postuning)\n",
    "res['pjaccard'] = res[['answer', 'postuned']].apply(jaccard, axis=1)\n",
    "res.jaccard.mean(), res.pjaccard.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file chaii-qa-pretraining_XLMR_68/checkpoint-7000/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file chaii-qa-pretraining_XLMR_68/checkpoint-7000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForQuestionAnswering.\n",
      "\n",
      "All the weights of XLMRobertaForQuestionAnswering were initialized from the model checkpoint at chaii-qa-pretraining_XLMR_68/checkpoint-7000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForQuestionAnswering for predictions without further training.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 6727\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='841' max='841' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [841/841 01:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing 371 example predictions split into 6727 features.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd3406256ab4b8cb404b9d1cb125b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/371 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.6879988647171935, 0.6911435188052438)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint = 'chaii-qa-pretraining_XLMR_68/checkpoint-7000'\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "raw_predictions = trainer.predict(valid_feats_small)\n",
    "final_predictions = postprocess_qa_predictions(chaii_valid_ds, validation_features, raw_predictions.predictions)\n",
    "res = pd.DataFrame(references)\n",
    "res['prediction'] = res['id'].apply(lambda r: final_predictions[r])\n",
    "res['jaccard'] = res[['answer', 'prediction']].apply(jaccard, axis=1)\n",
    "res['postuned'] = res['prediction'].apply(postuning)\n",
    "res['pjaccard'] = res[['answer', 'postuned']].apply(jaccard, axis=1)\n",
    "res.jaccard.mean(), res.pjaccard.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Uploading pretraining_XLMR_68/final/pytorch_model.bin\n",
      "Uploaded 1PSnVVGQ6wFqdE1dF-yEQAHm5yiidhbk9 at 3.0 MB/s, total 2.2 GB\n"
     ]
    }
   ],
   "source": [
    "!gdrive upload pretraining_XLMR_68/final/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Uploading pretraining_XLMR_68/final/config.json\n",
      "Uploaded 1a4Rc9LyDVx83MpMD2b-3QlgYBHlUcqyK at 613.0 B/s, total 718.0 B\n"
     ]
    }
   ],
   "source": [
    "!gdrive upload pretraining_XLMR_68/final/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "chaii-gpu-baseline-train-21 (pretraining).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0afc71e1269b4d82a373a18f011a0366": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4290b15ef9864a04b349d010e352984a",
       "IPY_MODEL_cb6e640c245d4119a2e14f8a2f3d047f",
       "IPY_MODEL_4430da2209f04060a0f158027a3cad78"
      ],
      "layout": "IPY_MODEL_44a880dc7324498fa503d1e7626ad7bb"
     }
    },
    "3c4b2c63d4f74feeaa2d90d897dbe894": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aeef95299b8c43b3b96a17822167ecef",
      "placeholder": "​",
      "style": "IPY_MODEL_627e5bae4c874b2a992c9cf62cbd2700",
      "value": "100%"
     }
    },
    "4290b15ef9864a04b349d010e352984a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b610168a856e4308a266ab05b29f10c0",
      "placeholder": "​",
      "style": "IPY_MODEL_bc5ee8afafec442697ab908b0cdd2911",
      "value": "100%"
     }
    },
    "4430da2209f04060a0f158027a3cad78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_90af0b691f3d490a88693155e501182a",
      "placeholder": "​",
      "style": "IPY_MODEL_e3b12b6d361446b5992d77777808ab04",
      "value": " 30933/30933 [00:37&lt;00:00, 941.19ex/s]"
     }
    },
    "44a880dc7324498fa503d1e7626ad7bb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4e892cabf058435590917913f0f23895": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5bde36aef5b545f48bdc96d2278e9156": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a27fa32ca6384107ac01cc91c2621ed2",
      "max": 20,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d14523fd472f461c99fb136594430001",
      "value": 20
     }
    },
    "627e5bae4c874b2a992c9cf62cbd2700": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8c7ab4d418d84eecb42a21875412463f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3c4b2c63d4f74feeaa2d90d897dbe894",
       "IPY_MODEL_5bde36aef5b545f48bdc96d2278e9156",
       "IPY_MODEL_b29d647ce6844a44949f1c8c0c646f84"
      ],
      "layout": "IPY_MODEL_9eb53fefb6944018aefaec00ca6b8fa4"
     }
    },
    "90af0b691f3d490a88693155e501182a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9eb53fefb6944018aefaec00ca6b8fa4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a27fa32ca6384107ac01cc91c2621ed2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a75561f0b48a418888864351cdc3b077": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae18d21de4f047f09e6fb864d2d3ca94": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aeef95299b8c43b3b96a17822167ecef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b29d647ce6844a44949f1c8c0c646f84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae18d21de4f047f09e6fb864d2d3ca94",
      "placeholder": "​",
      "style": "IPY_MODEL_4e892cabf058435590917913f0f23895",
      "value": " 20/20 [01:06&lt;00:00,  3.22s/it]"
     }
    },
    "b610168a856e4308a266ab05b29f10c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bc5ee8afafec442697ab908b0cdd2911": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "caf55231b4ff41429bb626801cdbb826": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cb6e640c245d4119a2e14f8a2f3d047f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a75561f0b48a418888864351cdc3b077",
      "max": 30933,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_caf55231b4ff41429bb626801cdbb826",
      "value": 30933
     }
    },
    "d14523fd472f461c99fb136594430001": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e3b12b6d361446b5992d77777808ab04": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
